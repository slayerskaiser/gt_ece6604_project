%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:

% Section I: Introduction

Current wireless high-speed data communication standards are considered to be 3.9G with 4G technologies deploying in the near future. Of the 3.9G technologies, Long Term Evolution (LTE) developed by the 3rd Generation Partnership Project (3GPP) is far more popular than IEEE 802.16e WiMAX. LTE is commonly marketed as 4G LTE even though it does not comply with ITU-R IMT-Advanced requirements to be considered 4G. Current 4G standards include 3GPP Long Term Evolution Advanced (LTE-A) and IEEE 802.16m WiMAX Release 2, with LTE-A being the more popular of the two currently. This delay in the development and deployment of actual 4G standards means that future 5G standards have a lot of catching up to do in order to meet the exponentially increasing number of mobile devices, each with greater data needs. To this end, this paper explores one possible method of meeting those needs, namely using massive MIMO systems with appropriately chosen space-time block codes.

The paper is organized as follows. The remainder of this section discusses the motivation and implementation of LTE and LTE-A. Section II provides background on diversity-increasing schemes used in current 3.9G and 4G standards, especially MIMO. Section III discusses current STBC schemes. Section IV explains the simulation methodology. The simulation results are presented and analyzed in section V. Finally, section VI summarizes the results and discusses how they can be applied to future 5G technologies. The simulation code used for this paper is included in the appendix.

\subsection{LTE}

LTE is a registered trademark owned by ETSI (European Telecommunications Standards Institute) for the wireless data communications technology and a development of the GSM/UMTS standards. LTE is based on the GSM/EDGE and UMTS/HSPA network technologies, increasing the capacity and speed using a different radio interface together with core network improvements. LTE is the natural upgrade path for carriers with both GSM/UMTS networks and CDMA2000 networks. The different LTE frequencies and bands used in different countries mean that only multi-band phones are be able to use LTE in all countries where it is supported.

The goal of LTE was to increase the capacity and speed of wireless data networks using new digital signal processing (DSP) techniques and modulations that were developed around the turn of the millennium. A further goal was the redesign and simplification of the network architecture to an IP-based system with significantly reduced transfer latency compared to the 3G architecture. The LTE wireless interface is incompatible with 2G and 3G networks, so it must be operated on a separate radio spectrum. The LTE specification provides downlink peak rates of 300~Mbit/s, uplink peak rates of 75~Mbit/s and QoS provisions permitting a transfer latency of less than 5ms in the radio access network. LTE has the ability to manage fast-moving mobiles and supports multi-cast and broadcast streams. LTE supports scalable carrier bandwidths, from 1.4~MHz to 20~MHz and supports both frequency division duplexing (FDD) and time-division duplexing (TDD). The IP-based network architecture, called the Evolved Packet Core (EPC) designed to replace the GPRS Core Network, supports seamless handovers for both voice and data to cell towers with older network technology such as GSM, UMTS and CDMA2000. The simpler architecture results in lower operating costs (for example, each E-UTRA cell will support up to four times the data and voice capacity supported by HSPA)

LTE supports deployment on different frequency bandwidths. The current specification outlines the following bandwidth blocks: 1.4MHz, 3MHz, 5MHz, 10MHz, 15MHz, and 20MHz. Frequency bandwidth blocks are essentially the amount of space a network operator dedicates to a network. Depending on the type of LTE being deployed, these bandwidths have slightly different meaning in terms of capacity. That will be covered later, though. An operator may choose to deploy LTE in a smaller bandwidth and grow it to a larger one as it transitions subscribers off of its legacy networks (GSM, CDMA, etc.).

LTE uses two different types of air interfaces (radio links), one for downlink (from tower to device), and one for uplink (from device to tower). By using different types of interfaces for the downlink and uplink, LTE utilizes the optimal way to do wireless connections both ways, which makes a better-optimized network and better battery life on LTE devices.

For the downlink, LTE uses an orthogonal frequency division multiple access (OFDMA) air interface as opposed to the code division multiple access (CDMA) and time division multiple access (TDMA) air interfaces . OFDMA (unlike CDMA and TDMA) mandates that multiple-input, multiple-output (MIMO) is used. Having MIMO means that devices have multiple connections to a single cell, which increases the stability of the connection and reduces latency tremendously. It also increases the total throughput of a connection. MIMO is what lets 802.11n WiFi reach speeds of up to 600Mbps, though most advertise up to 300-400Mbps. There is a significant disadvantage though. MIMO works better the further apart the individual carrier antennae are, in order to increase spatial diversity. On smaller phones, the interference caused by the antennae being so close to each other will cause LTE performance to drop. 

For the uplink (from device to tower), LTE uses the  discrete Fourier transform spread OFDMA (DFTS-OFDMA) scheme of generating a single carrier frequency division multiple access (SC-FDMA) signal. As opposed to regular OFDMA, SC-FDMA is better for uplink because it has a better peak-to-average power ratio over OFDMA for uplink. LTE-enabled devices, in order to conserve battery life, typically do not have a strong and powerful signal going back to the tower, so a lot of the benefits of normal OFDMA would be lost with a weak signal. Despite the name, SC-FDMA is still a MIMO system. LTE uses a SC-FDMA $1\times2$ configuration, which means that for every one antenna on the transmitting device, there are two antennae on the base station for receiving.

The LTE technology itself also comes in two flavors: an FDD (frequency division duplex) variant and a TDD (time division duplex) variant. The most common variant being used is the FDD variant. The FDD variant uses separate frequencies for downlink and uplink in the form of a band pair. That means for every band that a phone supports, it actually uses two frequency ranges. These are known as paired frequency bands. For example, Verizon’s 10MHz network is in FDD, so the bandwidth is allocated for uplink and downlink. This is commonly noted as a 2x10MHz or 10+10 MHz configuration. Some also call it 10x10MHz, but this is mathematically incorrect, but they mean 10+10MHz. Some will also call it a 20MHz network, but this can be ambiguous. The TDD variant uses one single range of frequencies in a frequency band, but that band is segmented to support transmit and receive signals in a single frequency range.

For example, an LTE TDD network deployed on 20MHz of spectrum uses the whole chunk as one large block for frequency allocation purposes. For network bandwidth purposes, a LTE TDD network’s spectrum can be further divided to optimize for the type of network traffic (half up and half down, mostly down and a bit up, mostly up and a bit down, and so on).

\subsection{LTE-A}

LTE-A is a major enhancement of the LTE standard that meets 4G requirements. It was standarized by 3GPP as 3GPP Release 10. Presently, there are few deployments and almost no capable smartphones.

The main developments of LTE-A over LTE are~\cite{TR36_912}
\begin{itemize}
\item Ability to use up to $8\times8$ MIMO and 128-QAM in downlink
\item Up to 100~MHz of carrier aggregated bandwidth
\item Higher spectral efficiency with a maximum of 30~bps/Hz in LTE-A release 10 versus 16~bps/Hz in release 8
\item 1~Gbit/s peak data rate per user
\item Increased peak data rate of 3~Gbit/s download and 1.5~Gbit/s upload
\item Faster switching between power states
\item Improved performance at cell edges
\end{itemize}
Even with these enhancements, LTE-A is backwards compatible with LTE, meaning an LTE mobile station would work in an LTE-A network and vice versa.

\subsection{Impact of LTE on Battery Life}

By itself, LTE devices should last roughly as long as their HSPA+ equivalents because of the optimized radios for both downlink and uplink operations. The reason why LTE devices right now eat batteries for breakfast is because the network operators are forcing many of these devices into active dual-mode operation.

For Verizon Wireless, this means that most of their LTE devices connect to both CDMA2000 and LTE simultaneously and stay connected to both. This consumes twice the amount of battery for every minute connected than if the phone were connected only to CDMA2000 or LTE. Additionally, when calls are made on Verizon Wireless LTE phones, the CDMA2000 radio sucks down more power because you are talking. Sending and receiving text messages causes pulses of CDMA2000 activity, which cuts battery life more. Arguably, constantly changing radio states could be worse for battery life than a switch into one mode for a period of time and switching back, so text messages may actually kill the batteries faster.

Also of importance for battery life is handover. Handover is the operation in which a device switches from one network to another or from one tower to another. Handover is the critical component that makes any cellular wireless network possible. Without handover, a user would have to manually select a new tower every time the user leaves the range of a tower. For cellular networks, this is even more critical because the range of a tower is not very predictable due to factors outside of anyone’s control (like the weather, etc.). LTE supports handover like all other cellular wireless networks, but it improves on it by doing it much faster when handing over to a supported type of network or cell.